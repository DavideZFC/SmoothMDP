{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.environments.PendulumSimple import PendulumSimple\n",
    "from classes.agents.FD_LSVI import FD_LSVI\n",
    "from functions.misc.test_algorithm_after_learning import test_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal design found: before squeezing 22.40609005246324, after squeezing 28.61723919050618\n",
      "(288, 3)\n",
      "(288, 20)\n",
      "(288,)\n",
      "(288, 2)\n",
      "(20,)\n",
      "step 19 done\n",
      "step 18 done\n",
      "step 17 done\n",
      "step 16 done\n",
      "step 15 done\n",
      "step 14 done\n",
      "step 13 done\n",
      "step 12 done\n",
      "step 11 done\n",
      "step 10 done\n",
      "step 9 done\n",
      "step 8 done\n",
      "step 7 done\n",
      "step 6 done\n",
      "step 5 done\n",
      "step 4 done\n",
      "step 3 done\n",
      "step 2 done\n",
      "step 1 done\n",
      "step 0 done\n"
     ]
    }
   ],
   "source": [
    "env = PendulumSimple()\n",
    "agent = FD_LSVI(env)\n",
    "state_disc = 40\n",
    "action_disc = 20\n",
    "agent.get_datasets(disc_numbers=[state_disc, state_disc, action_disc])\n",
    "agent.compute_q_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.98845875e-15, -2.06966111e-15, -2.98142397e-01, -1.54369897e-14,\n",
       "       -1.90736793e-14,  7.88453824e-15,  7.92309984e-15,  2.18044393e-14,\n",
       "       -7.47517556e-15, -1.30550067e-14, -2.96526679e-14, -2.98142397e-01,\n",
       "       -2.30802521e-14,  2.68243541e-14, -7.99788675e-15, -1.23575713e-14,\n",
       "        2.65287105e+01, -8.50478203e-15, -1.19256959e-02,  3.84275539e-14])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.w_vectors[-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state: [0. 0.], current action: [-0.05263158], current reward: 0.9999889196675901\n",
      "current state: [-0.0002513  -0.00197368], current action: [-0.05263158], current reward: 0.999988467797784\n",
      "current state: [-0.00054343 -0.00229441], current action: [-0.05263158], current reward: 0.9999881017710724\n",
      "current state: [-0.00085162 -0.00242054], current action: [-0.05263158], current reward: 0.9999876179624083\n",
      "current state: [-0.00117338 -0.00252707], current action: [-0.05263158], current reward: 0.9999869221891823\n",
      "current state: [-0.0015089  -0.00263516], current action: [-0.05263158], current reward: 0.9999859781754864\n",
      "current state: [-0.00185872 -0.00274748], current action: [-0.05263158], current reward: 0.9999847550128387\n",
      "current state: [-0.00222345 -0.00286455], current action: [-0.05263158], current reward: 0.9999832198534222\n",
      "current state: [-0.00260371 -0.00298661], current action: [-0.05263158], current reward: 0.9999813367670997\n",
      "current state: [-0.00300018 -0.00311386], current action: [-0.05263158], current reward: 0.9999790663404599\n",
      "current state: [-0.00341354 -0.00324653], current action: [-0.05263158], current reward: 0.9999763653435735\n",
      "current state: [-0.00384451 -0.00338485], current action: [-0.05263158], current reward: 0.9999731863797667\n",
      "current state: [-0.00429385 -0.00352907], current action: [-0.05263158], current reward: 0.9999694775058269\n",
      "current state: [-0.00476233 -0.00367942], current action: [-0.05263158], current reward: 0.9999651818186991\n",
      "current state: [-0.00525077 -0.00383618], current action: [-0.05263158], current reward: 0.9999602370055779\n",
      "current state: [-0.00576001 -0.00399962], current action: [-0.05263158], current reward: 0.9999545748541787\n",
      "current state: [-0.00629095 -0.00417001], current action: [-0.05263158], current reward: 0.9999481207197194\n",
      "current state: [-0.00684451 -0.00434765], current action: [-0.05263158], current reward: 0.9999407929448428\n",
      "current state: [-0.00742166 -0.00453286], current action: [-0.05263158], current reward: 0.999932502228392\n",
      "current state: [-0.00802338 -0.00472595], current action: [-0.05263158], current reward: 0.9999231509385985\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 20 is out of bounds for axis 0 with size 20",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m episodic_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# action = env.action_space.sample()\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent state: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, current action: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, current reward: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(state, action, reward))\n",
      "File \u001b[1;32mc:\\Users\\david\\OneDrive\\Documenti\\programming\\SmoothMDP\\SmoothMDP\\classes\\agents\\FD_LSVI.py:178\u001b[0m, in \u001b[0;36mFD_LSVI.choose_action\u001b[1;34m(self, state, h)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03mChooses which action to perform based on the current state\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m    _ (vector): action to be performed on the environment\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    177\u001b[0m variable_action_mesh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_next_state_action_feature_map(state)\n\u001b[1;32m--> 178\u001b[0m best_action_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(np\u001b[38;5;241m.\u001b[39mdot(variable_action_mesh, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw_vectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_grid[best_action_index]])\n",
      "\u001b[1;31mIndexError\u001b[0m: index 20 is out of bounds for axis 0 with size 20"
     ]
    }
   ],
   "source": [
    "state = env.reset()[0]\n",
    "done = False\n",
    "h = 0\n",
    "episodic_return = 0\n",
    "\n",
    "while not done:\n",
    "    # action = env.action_space.sample()\n",
    "    action = agent.choose_action(state, h)\n",
    "\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    print('current state: {}, current action: {}, current reward: {}'.format(state, action, reward))\n",
    "    inutile = input()\n",
    "    episodic_return += reward\n",
    "\n",
    "    done = terminated or truncated\n",
    "    state = next_state\n",
    "    h += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.168026350790523"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = 1355\n",
    "((x + np.pi) % (2 * np.pi)) - np.pi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
