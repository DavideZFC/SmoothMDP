{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.environments.PendulumSimple import PendulumSimple\n",
    "from classes.agents.FD_LSVI import FD_LSVI\n",
    "from functions.misc.test_algorithm_after_learning import test_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal design found: before squeezing 22.40609005246324, after squeezing 27.759650871833763\n",
      "(288, 3)\n",
      "(288, 20)\n",
      "(288,)\n",
      "(288, 2)\n",
      "(20,)\n",
      "step 199 done\n",
      "step 198 done\n",
      "step 197 done\n",
      "step 196 done\n",
      "step 195 done\n",
      "step 194 done\n",
      "step 193 done\n",
      "step 192 done\n",
      "step 191 done\n",
      "step 190 done\n",
      "step 189 done\n",
      "step 188 done\n",
      "step 187 done\n",
      "step 186 done\n",
      "step 185 done\n",
      "step 184 done\n",
      "step 183 done\n",
      "step 182 done\n",
      "step 181 done\n",
      "step 180 done\n",
      "step 179 done\n",
      "step 178 done\n",
      "step 177 done\n",
      "step 176 done\n",
      "step 175 done\n",
      "step 174 done\n",
      "step 173 done\n",
      "step 172 done\n",
      "step 171 done\n",
      "step 170 done\n",
      "step 169 done\n",
      "step 168 done\n",
      "step 167 done\n",
      "step 166 done\n",
      "step 165 done\n",
      "step 164 done\n",
      "step 163 done\n",
      "step 162 done\n",
      "step 161 done\n",
      "step 160 done\n",
      "step 159 done\n",
      "step 158 done\n",
      "step 157 done\n",
      "step 156 done\n",
      "step 155 done\n",
      "step 154 done\n",
      "step 153 done\n",
      "step 152 done\n",
      "step 151 done\n",
      "step 150 done\n",
      "step 149 done\n",
      "step 148 done\n",
      "step 147 done\n",
      "step 146 done\n",
      "step 145 done\n",
      "step 144 done\n",
      "step 143 done\n",
      "step 142 done\n",
      "step 141 done\n",
      "step 140 done\n",
      "step 139 done\n",
      "step 138 done\n",
      "step 137 done\n",
      "step 136 done\n",
      "step 135 done\n",
      "step 134 done\n",
      "step 133 done\n",
      "step 132 done\n",
      "step 131 done\n",
      "step 130 done\n",
      "step 129 done\n",
      "step 128 done\n",
      "step 127 done\n",
      "step 126 done\n",
      "step 125 done\n",
      "step 124 done\n",
      "step 123 done\n",
      "step 122 done\n",
      "step 121 done\n",
      "step 120 done\n",
      "step 119 done\n",
      "step 118 done\n",
      "step 117 done\n",
      "step 116 done\n",
      "step 115 done\n",
      "step 114 done\n",
      "step 113 done\n",
      "step 112 done\n",
      "step 111 done\n",
      "step 110 done\n",
      "step 109 done\n",
      "step 108 done\n",
      "step 107 done\n",
      "step 106 done\n",
      "step 105 done\n",
      "step 104 done\n",
      "step 103 done\n",
      "step 102 done\n",
      "step 101 done\n",
      "step 100 done\n",
      "step 99 done\n",
      "step 98 done\n",
      "step 97 done\n",
      "step 96 done\n",
      "step 95 done\n",
      "step 94 done\n",
      "step 93 done\n",
      "step 92 done\n",
      "step 91 done\n",
      "step 90 done\n",
      "step 89 done\n",
      "step 88 done\n",
      "step 87 done\n",
      "step 86 done\n",
      "step 85 done\n",
      "step 84 done\n",
      "step 83 done\n",
      "step 82 done\n",
      "step 81 done\n",
      "step 80 done\n",
      "step 79 done\n",
      "step 78 done\n",
      "step 77 done\n",
      "step 76 done\n",
      "step 75 done\n",
      "step 74 done\n",
      "step 73 done\n",
      "step 72 done\n",
      "step 71 done\n",
      "step 70 done\n",
      "step 69 done\n",
      "step 68 done\n",
      "step 67 done\n",
      "step 66 done\n",
      "step 65 done\n",
      "step 64 done\n",
      "step 63 done\n",
      "step 62 done\n",
      "step 61 done\n",
      "step 60 done\n",
      "step 59 done\n",
      "step 58 done\n",
      "step 57 done\n",
      "step 56 done\n",
      "step 55 done\n",
      "step 54 done\n",
      "step 53 done\n",
      "step 52 done\n",
      "step 51 done\n",
      "step 50 done\n",
      "step 49 done\n",
      "step 48 done\n",
      "step 47 done\n",
      "step 46 done\n",
      "step 45 done\n",
      "step 44 done\n",
      "step 43 done\n",
      "step 42 done\n",
      "step 41 done\n",
      "step 40 done\n",
      "step 39 done\n",
      "step 38 done\n",
      "step 37 done\n",
      "step 36 done\n",
      "step 35 done\n",
      "step 34 done\n",
      "step 33 done\n",
      "step 32 done\n",
      "step 31 done\n",
      "step 30 done\n",
      "step 29 done\n",
      "step 28 done\n",
      "step 27 done\n",
      "step 26 done\n",
      "step 25 done\n",
      "step 24 done\n",
      "step 23 done\n",
      "step 22 done\n",
      "step 21 done\n",
      "step 20 done\n",
      "step 19 done\n",
      "step 18 done\n",
      "step 17 done\n",
      "step 16 done\n",
      "step 15 done\n",
      "step 14 done\n",
      "step 13 done\n",
      "step 12 done\n",
      "step 11 done\n",
      "step 10 done\n",
      "step 9 done\n",
      "step 8 done\n",
      "step 7 done\n",
      "step 6 done\n",
      "step 5 done\n",
      "step 4 done\n",
      "step 3 done\n",
      "step 2 done\n",
      "step 1 done\n",
      "step 0 done\n"
     ]
    }
   ],
   "source": [
    "env = PendulumSimple()\n",
    "agent = FD_LSVI(env)\n",
    "state_disc = 40\n",
    "action_disc = 20\n",
    "agent.get_datasets(disc_numbers=[state_disc, state_disc, action_disc])\n",
    "agent.compute_q_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state: [0. 0.], current action: [1.], current reward: -0.004\n",
      "current state: [0.00477465 0.0375    ], current action: [1.], current reward: -0.004365625000000004\n",
      "current state: [0.01032517 0.0435937 ], current action: [1.], current reward: -0.00524223108194267\n",
      "current state: [0.01618076 0.04598969], current action: [1.], current reward: -0.006795535256655696\n",
      "current state: [0.02229387 0.04801229], current action: [1.], current reward: -0.009135877305659583\n",
      "current state: [0.028668   0.05006226], current action: [1.], current reward: -0.012361998134885284\n",
      "current state: [0.03531301 0.05218979], current action: [1.], current reward: -0.016579859401593892\n",
      "current state: [0.04223981 0.05440294], current action: [1.], current reward: -0.0219053292486416\n",
      "current state: [0.04945965 0.05670454], current action: [1.], current reward: -0.02846513238625359\n",
      "current state: [0.05698406 0.05909659], current action: [1.], current reward: -0.03639765898374765\n",
      "current state: [0.06482477 0.06158077], current action: [1.], current reward: -0.04585377401376117\n",
      "current state: [0.07299367 0.06415837], current action: [1.], current reward: -0.05699762797074641\n",
      "current state: [0.08150277 0.06683028], current action: [1.], current reward: -0.07000745493293746\n",
      "current state: [0.09036411 0.06959689], current action: [1.], current reward: -0.08507633933786289\n",
      "current state: [0.09958976 0.07245799], current action: [1.], current reward: -0.10241292845038602\n",
      "current state: [0.10919159 0.07541265], current action: [1.], current reward: -0.12224206257357474\n",
      "current state: [0.11918132 0.07845914], current action: [1.], current reward: -0.1448052895876889\n",
      "current state: [0.12957029 0.08159477], current action: [1.], current reward: -0.17036122448240482\n",
      "current state: [0.14036937 0.08481578], current action: [1.], current reward: -0.19918570831165017\n",
      "current state: [0.1515888  0.08811719], current action: [1.], current reward: -0.2315717146649705\n",
      "current state: [0.163238   0.09149266], current action: [1.], current reward: -0.2678289456251854\n",
      "current state: [0.17532542 0.09493434], current action: [1.], current reward: -0.3082830536922834\n",
      "current state: [0.18785827 0.09843273], current action: [1.], current reward: -0.3532744218474915\n",
      "current state: [0.20084232 0.1019765 ], current action: [1.], current reward: -0.4031564314922374\n",
      "current state: [0.21428166 0.10555244], current action: [1.], current reward: -0.45829314823729866\n",
      "current state: [0.22817849 0.10914529], current action: [1.], current reward: -0.5190563593605717\n",
      "current state: [0.24253269 0.11273772], current action: [1.], current reward: -0.5858219051870905\n",
      "current state: [0.25734177 0.11631023], current action: [1.], current reward: -0.6589652606571859\n",
      "current state: [0.27260044 0.11984127], current action: [1.], current reward: -0.7388563438141698\n",
      "current state: [0.2883004  0.12330721], current action: [1.], current reward: -0.8258535554907721\n",
      "current state: [0.30443013 0.12668258], current action: [1.], current reward: -0.9202970893180372\n",
      "current state: [0.32097462 0.12994023], current action: [1.], current reward: -1.022501592935191\n",
      "current state: [0.3379153  0.13305171], current action: [1.], current reward: -1.132748308775645\n",
      "current state: [0.3552298  0.13598765], current action: [1.], current reward: -1.251276873945555\n",
      "current state: [0.37289193 0.13871829], current action: [1.], current reward: -1.378277010399503\n",
      "current state: [0.39087188 0.141214  ], current action: [1.], current reward: -1.5138803848020372\n",
      "current state: [0.40913597 0.14344597], current action: [1.], current reward: -1.6581529573428824\n",
      "current state: [0.42764723 0.14538695], current action: [1.], current reward: -1.811088165180003\n",
      "current state: [0.44636536 0.1470119 ], current action: [1.], current reward: -1.9726012941395663\n",
      "current state: [0.46524733 0.14829877], current action: [1.], current reward: -2.142525377702673\n",
      "current state: [0.4842478  0.14922915], current action: [1.], current reward: -2.3206089226567834\n",
      "current state: [0.5033195  0.14978887], current action: [1.], current reward: -2.506515695843863\n",
      "current state: [0.5224141 0.1499685], current action: [1.], current reward: -2.6998267186466656\n",
      "current state: [0.54148257 0.14976373], current action: [1.], current reward: -2.9000445104549284\n",
      "current state: [0.5604762  0.14917548], current action: [1.], current reward: -3.1065995070368793\n",
      "current state: [0.5793469  0.14820997], current action: [1.], current reward: -3.318858463905949\n",
      "current state: [0.59804803 0.14687857], current action: [1.], current reward: -3.536134548428972\n",
      "current state: [0.6165351  0.14519735], current action: [1.], current reward: -3.7576987369301493\n",
      "current state: [0.6347662 0.1431867], current action: [1.], current reward: -3.98279207189646\n",
      "current state: [0.65270245 0.1408707 ], current action: [1.], current reward: -4.210638304251473\n",
      "current state: [0.67030835 0.13827634], current action: [1.], current reward: -4.44045644785636\n",
      "current state: [0.6875522  0.13543284], current action: [1.], current reward: -4.67147280588279\n",
      "current state: [0.7044062  0.13237084], current action: [1.], current reward: -4.902932086566124\n",
      "current state: [0.7208465  0.12912168], current action: [1.], current reward: -5.13410730213975\n",
      "current state: [0.7368532  0.12571666], current action: [1.], current reward: -5.3643082315283195\n",
      "current state: [0.7524105  0.12218648], current action: [1.], current reward: -5.592888316739298\n",
      "current state: [0.76750606 0.11856067], current action: [1.], current reward: -5.8192499478589985\n",
      "current state: [0.78213143 0.11486714], current action: [1.], current reward: -6.042848166679581\n",
      "current state: [0.79628116 0.11113186], current action: [1.], current reward: -6.263192880630674\n",
      "current state: [0.80995303 0.10737868], current action: [1.], current reward: -6.479849725075184\n",
      "current state: [0.82314754 0.10362909], current action: [1.], current reward: -6.692439742990003\n",
      "current state: [0.83586746 0.09990225], current action: [1.], current reward: -6.900638067698935\n",
      "current state: [0.84811795 0.09621494], current action: [1.], current reward: -7.104171798632609\n",
      "current state: [0.8599058  0.09258163], current action: [1.], current reward: -7.302817254501842\n",
      "current state: [0.8712395  0.08901462], current action: [1.], current reward: -7.4963967753589715\n",
      "current state: [0.8821288  0.08552413], current action: [1.], current reward: -7.684775227231268\n",
      "current state: [0.8925844  0.08211849], current action: [1.], current reward: -7.867856342490415\n",
      "current state: [0.90261805 0.07880428], current action: [1.], current reward: -8.045579007633016\n",
      "current state: [0.91224205 0.07558656], current action: [1.], current reward: -8.217913589039316\n",
      "current state: [0.9214691  0.07246897], current action: [1.], current reward: -8.384858367505666\n",
      "current state: [0.9303122  0.06945396], current action: [1.], current reward: -8.546436134519864\n",
      "current state: [0.9387847  0.06654292], current action: [1.], current reward: -8.702690987693831\n",
      "current state: [0.9468999  0.06373631], current action: [1.], current reward: -8.853685349589185\n",
      "current state: [0.95467097 0.06103387], current action: [1.], current reward: -8.999497223316018\n",
      "current state: [0.9621111  0.05843465], current action: [1.], current reward: -9.140217689593426\n",
      "current state: [0.9692333 0.0559372], current action: [1.], current reward: -9.275948643210112\n",
      "current state: [0.97605014 0.0535396 ], current action: [1.], current reward: -9.406800761761346\n",
      "current state: [0.98257416 0.05123961], current action: [1.], current reward: -9.532891695906718\n",
      "current state: [0.98881745 0.04903472], current action: [1.], current reward: -9.654344467942288\n",
      "current state: [0.99479175 0.04692219], current action: [1.], current reward: -9.77128606398325\n",
      "current state: [1.0005085  0.04489915], current action: [1.], current reward: -9.863770888972077\n",
      "current state: [1.0059787  0.04296263], current action: [1.], current reward: -9.756127254951833\n",
      "current state: [1.011213   0.04110956], current action: [1.], current reward: -9.653680188319923\n",
      "current state: [1.0162214 0.0393369], current action: [1.], current reward: -9.556157651956795\n",
      "current state: [1.0210141  0.03764157], current action: [1.], current reward: -9.463302327112014\n",
      "current state: [1.0256004  0.03602052], current action: [1.], current reward: -9.37487093437098\n",
      "current state: [1.0299894  0.03447075], current action: [1.], current reward: -9.290633553994976\n",
      "current state: [1.0341897  0.03298931], current action: [1.], current reward: -9.210372953339359\n",
      "current state: [1.0382097  0.03157333], current action: [1.], current reward: -9.133883927331608\n",
      "current state: [1.0420574  0.03021998], current action: [1.], current reward: -9.060972656554359\n",
      "current state: [1.0457405  0.02892657], current action: [1.], current reward: -8.991456086289192\n",
      "current state: [1.0492661  0.02769045], current action: [1.], current reward: -8.925161328900401\n",
      "current state: [1.0526414  0.02650909], current action: [1.], current reward: -8.86192509114214\n",
      "current state: [1.0558729  0.02538004], current action: [1.], current reward: -8.801593127329332\n",
      "current state: [1.058967   0.02430096], current action: [1.], current reward: -8.744019718798906\n",
      "current state: [1.0619297  0.02326958], current action: [1.], current reward: -8.689067179681416\n",
      "current state: [1.064767   0.02228373], current action: [1.], current reward: -8.636605388686023\n",
      "current state: [1.0674843  0.02134136], current action: [1.], current reward: -8.58651134635909\n",
      "current state: [1.0700868  0.02044047], current action: [1.], current reward: -8.538668757094324\n",
      "current state: [1.0725797  0.01957917], current action: [1.], current reward: -8.492967635040001\n",
      "current state: [1.0749677  0.01875565], current action: [1.], current reward: -8.4493039329567\n",
      "current state: [1.0772556  0.01796818], current action: [1.], current reward: -8.407579193019187\n",
      "current state: [1.0794474  0.01721512], current action: [1.], current reward: -8.367700218522552\n",
      "current state: [1.0815476  0.01649488], current action: [1.], current reward: -8.32957876543938\n",
      "current state: [1.0835601  0.01580596], current action: [1.], current reward: -8.293131252777588\n",
      "current state: [1.0854887  0.01514695], current action: [1.], current reward: -8.258278490704066\n",
      "current state: [1.087337   0.01451647], current action: [1.], current reward: -8.224945425423556\n",
      "current state: [1.0891085  0.01391323], current action: [1.], current reward: -8.193060899834\n",
      "current state: [1.0908065  0.01333599], current action: [1.], current reward: -8.162557429015989\n",
      "current state: [1.0924342  0.01278358], current action: [1.], current reward: -8.133370989653752\n",
      "current state: [1.0939945  0.01225486], current action: [1.], current reward: -8.105440822527047\n",
      "current state: [1.0954903  0.01174878], current action: [1.], current reward: -8.078709247256207\n",
      "current state: [1.0969245  0.01126431], current action: [1.], current reward: -8.053121488525935\n",
      "current state: [1.0982997  0.01080049], current action: [1.], current reward: -8.028625513056038\n",
      "current state: [1.0996183  0.01035639], current action: [1.], current reward: -8.005171876629573\n",
      "current state: [1.1008828  0.00993113], current action: [1.], current reward: -7.982713580529644\n",
      "current state: [1.1020955  0.00952387], current action: [1.], current reward: -7.961205936775475\n",
      "current state: [1.1032584  0.00913382], current action: [1.], current reward: -7.940606441586288\n",
      "current state: [1.1043738  0.00876021], current action: [1.], current reward: -7.920874656537715\n",
      "current state: [1.1054436  0.00840232], current action: [1.], current reward: -7.901972096909672\n",
      "current state: [1.1064698  0.00805946], current action: [1.], current reward: -7.883862126757328\n",
      "current state: [1.1074541  0.00773096], current action: [1.], current reward: -7.866509860267483\n",
      "current state: [1.1083983 0.0074162], current action: [1.], current reward: -7.84988206899171\n",
      "current state: [1.1093042  0.00711458], current action: [1.], current reward: -7.833947094574787\n",
      "current state: [1.1101732  0.00682553], current action: [1.], current reward: -7.818674766622588\n",
      "current state: [1.1110071 0.0065485], current action: [1.], current reward: -7.804036325377574\n",
      "current state: [1.111807   0.00628298], current action: [1.], current reward: -7.790004348892394\n",
      "current state: [1.1125746  0.00602846], current action: [1.], current reward: -7.77655268441317\n",
      "current state: [1.113311   0.00578448], current action: [1.], current reward: -7.763656383703427\n",
      "current state: [1.1140178  0.00555057], current action: [1.], current reward: -7.751291642058188\n",
      "current state: [1.114696   0.00532632], current action: [1.], current reward: -7.7394357407743755\n",
      "current state: [1.1153468 0.0051113], current action: [1.], current reward: -7.728066992859952\n",
      "current state: [1.1159713  0.00490513], current action: [1.], current reward: -7.717164691778611\n",
      "current state: [1.1165707  0.00470742], current action: [1.], current reward: -7.7067090630408375\n",
      "current state: [1.1171459  0.00451783], current action: [1.], current reward: -7.696681218464835\n",
      "current state: [1.117698 0.004336], current action: [1.], current reward: -7.687063112942682\n",
      "current state: [1.1182278  0.00416161], current action: [1.], current reward: -7.677837503558219\n",
      "current state: [1.1187364  0.00399435], current action: [1.], current reward: -7.668987910913435\n",
      "current state: [1.1192245  0.00383391], current action: [1.], current reward: -7.660498582529599\n",
      "current state: [1.1196932  0.00368001], current action: [1.], current reward: -7.6523544581983876\n",
      "current state: [1.1201429  0.00353237], current action: [1.], current reward: -7.644541137166488\n",
      "current state: [1.1205746  0.00339075], current action: [1.], current reward: -7.63704484704474\n",
      "current state: [1.1209891  0.00325487], current action: [1.], current reward: -7.629852414340257\n",
      "current state: [1.1213869  0.00312451], current action: [1.], current reward: -7.622951236516297\n",
      "current state: [1.1217687  0.00299944], current action: [1.], current reward: -7.616329255491109\n",
      "current state: [1.1221354  0.00287943], current action: [1.], current reward: -7.6099749324926185\n",
      "current state: [1.1224873  0.00276428], current action: [1.], current reward: -7.603877224191057\n",
      "current state: [1.1228253  0.00265378], current action: [1.], current reward: -7.5980255600368265\n",
      "current state: [1.1231496  0.00254775], current action: [1.], current reward: -7.592409820735395\n",
      "current state: [1.123461 0.002446], current action: [1.], current reward: -7.587020317795252\n",
      "current state: [1.1237601  0.00234835], current action: [1.], current reward: -7.581847774089178\n",
      "current state: [1.1240472  0.00225464], current action: [1.], current reward: -7.576883305372584\n",
      "current state: [1.1243228 0.0021647], current action: [1.], current reward: -7.572118402706418\n",
      "current state: [1.1245874  0.00207838], current action: [1.], current reward: -7.56754491573507\n",
      "current state: [1.1248415  0.00199554], current action: [1.], current reward: -7.563155036773004\n",
      "current state: [1.1250854  0.00191602], current action: [1.], current reward: -7.558941285656591\n",
      "current state: [1.1253196  0.00183969], current action: [1.], current reward: -7.554896495320109\n",
      "current state: [1.1255445  0.00176643], current action: [1.], current reward: -7.551013798057536\n",
      "current state: [1.1257606  0.00169611], current action: [1.], current reward: -7.547286612433936\n",
      "current state: [1.1259679  0.00162861], current action: [1.], current reward: -7.543708630812433\n",
      "current state: [1.1261669  0.00156381], current action: [1.], current reward: -7.540273807464694\n",
      "current state: [1.1263582  0.00150161], current action: [1.], current reward: -7.5369763472348215\n",
      "current state: [1.1265417  0.00144189], current action: [1.], current reward: -7.533810694728193\n",
      "current state: [1.126718   0.00138457], current action: [1.], current reward: -7.530771523998528\n",
      "current state: [1.1268873  0.00132954], current action: [1.], current reward: -7.527853728707936\n",
      "current state: [1.1270499 0.0012767], current action: [1.], current reward: -7.525052412736057\n",
      "current state: [1.127206   0.00122598], current action: [1.], current reward: -7.522362881215968\n",
      "current state: [1.1273558  0.00117729], current action: [1.], current reward: -7.519780631975523\n",
      "current state: [1.1274998  0.00113053], current action: [1.], current reward: -7.51730134736416\n",
      "current state: [1.1276381e+00 1.0856461e-03], current action: [1.], current reward: -7.514920886446252\n",
      "current state: [1.1277708e+00 1.0425494e-03], current action: [1.], current reward: -7.512635277543051\n",
      "current state: [1.1278982e+00 1.0011711e-03], current action: [1.], current reward: -7.510440711106454\n",
      "current state: [1.1280206e+00 9.6144207e-04], current action: [1.], current reward: -7.508333532908415\n",
      "current state: [1.1281382e+00 9.2329603e-04], current action: [1.], current reward: -7.506310237530969\n",
      "current state: [1.1282511e+00 8.8666944e-04], current action: [1.], current reward: -7.504367462142554\n",
      "current state: [1.1283596e+00 8.5150125e-04], current action: [1.], current reward: -7.502501980546969\n",
      "current state: [1.1284636e+00 8.1773306e-04], current action: [1.], current reward: -7.500710697492153\n",
      "current state: [1.1285636e+00 7.8530866e-04], current action: [1.], current reward: -7.498990643226629\n",
      "current state: [1.1286597e+00 7.5417425e-04], current action: [-1.], current reward: -7.497338968291967\n",
      "current state: [ 1.1192026  -0.07427572], current action: [-1.], current reward: -7.6614309741629105\n",
      "current state: [ 1.1088794  -0.08107767], current action: [-1.], current reward: -7.842068869002181\n",
      "current state: [ 1.0988106  -0.07908068], current action: [-1.], current reward: -8.020148960206974\n",
      "current state: [ 1.0891311  -0.07602207], current action: [-1.], current reward: -8.193211327100217\n",
      "current state: [ 1.0798477  -0.07291234], current action: [-1.], current reward: -8.360931146151945\n",
      "current state: [ 1.0709496  -0.06988527], current action: [-1.], current reward: -8.523285667834022\n",
      "current state: [ 1.0624241  -0.06695944], current action: [-1.], current reward: -8.680310509256635\n",
      "current state: [ 1.0542578  -0.06413771], current action: [-1.], current reward: -8.832064869401696\n",
      "current state: [ 1.0464375  -0.06142018], current action: [-1.], current reward: -8.978624550925847\n",
      "current state: [ 1.0389501  -0.05880604], current action: [-1.], current reward: -9.120078867303935\n",
      "current state: [ 1.0317825  -0.05629391], current action: [-1.], current reward: -9.25652827155265\n",
      "current state: [ 1.0249221  -0.05388193], current action: [-1.], current reward: -9.388082282129876\n",
      "current state: [ 1.0183563  -0.05156791], current action: [-1.], current reward: -9.514857640494773\n",
      "current state: [ 1.0120729  -0.04934936], current action: [-1.], current reward: -9.63697668012343\n",
      "current state: [ 1.0060602  -0.04722359], current action: [-1.], current reward: -9.754565891480421\n",
      "current state: [ 1.0003067  -0.04518772], current action: [-1.], current reward: -9.867754667437392\n",
      "current state: [ 0.9948014  -0.04323881], current action: [-1.], current reward: -9.771441967109643\n",
      "current state: [ 0.98953354 -0.04137381], current action: [-1.], current reward: -9.668256924265947\n",
      "current state: [ 0.98449284 -0.03958965], current action: [-1.], current reward: -9.570035017343233\n",
      "current state: [ 0.9796694  -0.03788327], current action: [-1.], current reward: -9.476516830580858\n",
      "current state: [ 0.97505367 -0.03625162], current action: [-0.05263158], current reward: -9.383468166817899\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()[0]\n",
    "done = False\n",
    "h = 0\n",
    "episodic_return = 0\n",
    "\n",
    "while not done:\n",
    "    # action = env.action_space.sample()\n",
    "    action = agent.choose_action(state, h)\n",
    "\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    print('current state: {}, current action: {}, current reward: {}'.format(state, action, reward))\n",
    "    inutile = input()\n",
    "    episodic_return += reward\n",
    "\n",
    "    done = terminated or truncated\n",
    "    state = next_state\n",
    "    h += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
